{
  "n_epochs": 100,
  "lr": 0.001,
  "batch_size": 32,
  "wandb": false,
  "device": "cuda",

  "model": {
    "n_classes": 6,
    "embedding_dim": 256,
    "hidden_dim": 64,
    "dropout": 0.2,
    "n_layers": 2,
    "bidirectional": true
  },

  "save": "models/bilstm_2layers.pth",
  "vocab": "models/vocab_bilstm_2layers.pth"
}